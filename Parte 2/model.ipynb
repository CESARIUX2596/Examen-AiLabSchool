{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guitar Image Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is part of the AiLab School AI cours. Contains the code to train a model to classify guitar images, part of the final project of the course.3\n",
    "\n",
    "The dataset used in this project is derivated from the publication Fuzzy Edge-Detection as a Preprocessing Layer in Deep Neural Networks for Guitar Classification, contains images of 6 different guitar types.\n",
    "<li>Acoustic</li>\n",
    "<li>Double Cut</li>\n",
    "<li>Single Cut</li>\n",
    "<li>S-Style</li>\n",
    "<li>T-Style</li>\n",
    "<li>Ukulele</li>\n",
    "The dataset was created from a web search and contains 900 images of each guitar type. The images were resized to 224x224 pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow imports for CNN model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from utils import ProcessingUtils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if gpus are available to tensorflow\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(\"Num CPUs Available: \", len(tf.config.experimental.list_physical_devices('CPU')))\n",
    "\n",
    "print('='*90)\n",
    "# check gpus specs with nvidia-smi\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "DATA_PATH = 'Y://Datasets//CGD//CGD_no_split//'\n",
    "CLASES = {0: 'Acoustic', 1: 'Double_cut', 2: 'Single_cut', 3: 'S_style', 4: 'T_style', 5: 'Ukulele'}\n",
    "class_idx = [0, 1, 2, 3, 4, 5]\n",
    "class_idx = [str(i) for i in class_idx]\n",
    "# X,y = ProcessingUtils.load_dataset(path=DATA_PATH, classes=class_idx, img_size=(224, 224), shuffle=True, seed=1313, verbose=True)\n",
    "X,y = ProcessingUtils.load_dataset(path=DATA_PATH, classes=class_idx, img_size=(150,150), shuffle=True, seed=1313, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessingUtils.sample_dataset(X, y, CLASES, n=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "As que can apreciate, the dataset contains noise (images that contain items besides guitars), and images that are not centered in the guitar. This can be a problem for the model training.\n",
    "The dataset is balanced, contains 900 images of each guitar type.\n",
    "The guitar styles (cathegories) present similarities with other styles, for example, the acoustic guitar is similar to the ukulele, and the double cut is similar to the single cut, T-Style and S-Style. This may pressent a problem for the model training.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ProcessingUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "X = ProcessingUtils.normalize_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, X_val, y_train, y_test, y_val = ProcessingUtils.split_data(X, y, train_size=0.7, test_size=0.2, val_size=0.1, seed=1313)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of the data\n",
    "print('Train data shape:', X_train.shape)\n",
    "print('Train labels shape:', y_train.shape)\n",
    "print('Test data shape:', X_test.shape)\n",
    "print('Test labels shape:', y_test.shape)\n",
    "print('Val data shape:', X_val.shape)\n",
    "print('Val labels shape:', y_val.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create convolution blocks\n",
    "class ConvBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides, padding, pool_size, pool_strides, pool_padding, activation):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, activation=activation)\n",
    "        self.pool = MaxPooling2D(pool_size=pool_size, strides=pool_strides, padding=pool_padding)\n",
    "        self.batch_norm = BatchNormalization()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        x = self.pool(x)\n",
    "        x = self.batch_norm(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "    \n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Create the model\n",
    "\n",
    "model = Sequential([\n",
    "    ConvBlock(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "    ConvBlock(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "    ConvBlock(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "    ConvBlock(filters=128, kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "    Dropout(0.75),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(6, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilize tensorboard for visualization\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), callbacks=[tensorboard_callback])\n",
    "\n",
    "# early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, verbose=2, mode='min', restore_best_weights=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), verbose=2.  callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "#confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=CLASES.values(), yticklabels=CLASES.values())\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the accuracy and loss curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy and Loss Curves')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('./models/base_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the session and delete the model to free memory\n",
    "tf.keras.backend.clear_session()\n",
    "del model\n",
    "del X\n",
    "del y\n",
    "del history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tunning\n",
    "Utilize hyperopt to find the best hyperparameters for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt and mlflow imports\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "import mlflow.keras\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "from hyperopt import space_eval\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "mlflow.set_experiment(\"Guitar Classification\")\n",
    "\n",
    "# define the search space\n",
    "space = {\n",
    "    'conv1_filters': hp.choice('conv1_filters', [32, 64, 128, 256]),\n",
    "    'conv2_filters': hp.choice('conv2_filters', [32, 64, 128, 256]),\n",
    "    'conv3_filters': hp.choice('conv3_filters', [32, 64, 128, 256]),\n",
    "    'conv4_filters': hp.choice('conv4_filters', [32, 64, 128, 256]),\n",
    "    'dropout': hp.uniform('dropout', 0.1, 0.9),\n",
    "    'dense': hp.choice('dense', [256, 512, 1024]),\n",
    "    'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "    'epochs': hp.choice('epochs', [50, 100, 150]),\n",
    "    'learning_rate': hp.choice('learning_rate',[0.0001, 0.001, 0.01, 0.1])\n",
    "}\n",
    "\n",
    "# define the objective function to optimize f1_score, accuracy, precision, recall\n",
    "def objective(space):\n",
    "    with mlflow.start_run():\n",
    "        model = Sequential([\n",
    "            ConvBlock(filters=space['conv1_filters'], kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "            ConvBlock(filters=space['conv2_filters'], kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "            ConvBlock(filters=space['conv3_filters'], kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "            ConvBlock(filters=space['conv4_filters'], kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "            Dropout(space['dropout']),\n",
    "            Flatten(),\n",
    "            Dense(space['dense'], activation='relu'),\n",
    "            Dense(6, activation='sigmoid')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=space['learning_rate']), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        model = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), verbose=0, callbacks=[early_stopping])\n",
    "        loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        mlflow.log_metric('loss', loss)\n",
    "        mlflow.log_metric('accuracy', accuracy)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        mlflow.log_metric('f1', f1)\n",
    "        mlflow.log_metric('precision', precision)\n",
    "        mlflow.log_metric('recall', recall)\n",
    "        mlflow.keras.log_model(model, 'model')\n",
    "        mlflow.log_metric('epochs', space['epochs'])\n",
    "        mlflow.log_metric('batch_size', space['batch_size'])\n",
    "        mlflow.log_metric('learning_rate', space['learning_rate'])\n",
    "        mlflow.log_metric('conv1_filters', space['conv1_filters'])\n",
    "        mlflow.log_metric('conv2_filters', space['conv2_filters'])\n",
    "        mlflow.log_metric('conv3_filters', space['conv3_filters'])\n",
    "        mlflow.log_metric('conv4_filters', space['conv4_filters'])\n",
    "        return {'loss': loss, 'status': STATUS_OK, 'model': model}\n",
    "    \n",
    "# define the trials object\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, \n",
    "            space=space, \n",
    "            algo=tpe.suggest, \n",
    "            max_evals=500, \n",
    "            trials=trials)\n",
    "\n",
    "            \n",
    "# get the best hyperparameters\n",
    "best_params = space_eval(space, best)\n",
    "\n",
    "# create the best model\n",
    "hyper_params_dict = {\n",
    "    'conv1_filters': [32, 64, 128, 256],\n",
    "    'conv2_filters': [32, 64, 128, 256],\n",
    "    'conv3_filters': [32, 64, 128, 256],\n",
    "    'conv4_filters': [32, 64, 128, 256],\n",
    "    'dropout': range(0.1, 0.9),\n",
    "    'dense': [256, 512, 1024],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [50, 100, 150],\n",
    "    'learning_rate': [0.0001, 0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "\n",
    "# create the best model\n",
    "best_model = Sequential([\n",
    "    ConvBlock(filters=best_params['conv1_filters'], kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "    ConvBlock(filters=best_params['conv2_filters'], kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "    ConvBlock(filters=best_params['conv3_filters'], kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "    ConvBlock(filters=best_params['conv4_filters'], kernel_size=(3, 3), strides=(1, 1), padding='same', pool_size=(2, 2), pool_strides=(2, 2), pool_padding='valid', activation='relu'),\n",
    "    Dropout(best_params['dropout']),\n",
    "    Flatten(),\n",
    "    Dense(best_params['dense'], activation='relu'),\n",
    "    Dense(6, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile the best model\n",
    "best_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=best_params['learning_rate']), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the best model\n",
    "history = best_model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_val, y_val), verbose=2, callbacks=[early_stopping])\n",
    "\n",
    "# evaluate the best model\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# seaborn confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure(figsize=(10, 10))\n",
    "labels = CLASES.keys()\n",
    "sns.heatmap(cm, annot=True, square=True, cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "# save the best model with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "best_model.save(f'./models/{timestamp}_best_model.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "als_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
